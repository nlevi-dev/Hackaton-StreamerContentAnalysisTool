This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
source/
  data_acquisition/
    cookies.txt
    download-data.py
    preprocess-videos.py
    pyproject.toml
    videos.txt
  feature_extraction/
    audio/
      extract.py
      pyproject.toml
    image/
      extract.py
      pyproject.toml
  frontend/
    first_video_data.py
    frontend.py
    top_features.csv
  label_extraction/
    extract_chat_metadata.py
  training/
    merge_video_and_labels.py
    preprocess.py
    train_rules.py
  pyproject.toml
.gitignore
gitpush.sh
mkdocs.yml
pyproject.toml
README.md

================================================================
Files
================================================================

================
File: source/data_acquisition/cookies.txt
================
# Netscape HTTP Cookie File
# This file is generated by yt-dlp.  Do not edit.

.youtube.com	TRUE	/	TRUE	1775951402	__Secure-1PAPISID	LaNWPo4ApXF469R7/AInNpn8a8Jb48eLib
.youtube.com	TRUE	/	TRUE	1775951402	__Secure-1PSID	g.a000uggwjpjV8EfIwqzfQSYerkpCdpP9J97NW4akZcSosHPNlqB7JjIQVDQvIAz_3H5_vHOaHAACgYKAcoSARESFQHGX2MiNCNhznRXFXz5GgWnwH_1PhoVAUF8yKoqQLEBMyhaKiLOpQFm8uH90076
.youtube.com	TRUE	/	TRUE	1772930082	__Secure-1PSIDCC	AKEyXzVru6W8LSqf_yQsA7sweDHTNQfcjI7H43kbQiA4msOuzR9RElwSa5TA0Bm5XMWnsYzX3g
.youtube.com	TRUE	/	TRUE	1772927402	__Secure-1PSIDTS	sidts-CjEBEJ3XV4zWBqC5YMY9TxOWiJp7U1ze9Pt9cX0Aefl1Gmn3RSx2xZb7o9T2z8sPHxTSEAA
.youtube.com	TRUE	/	TRUE	1775951402	__Secure-3PAPISID	LaNWPo4ApXF469R7/AInNpn8a8Jb48eLib
.youtube.com	TRUE	/	TRUE	1775951402	__Secure-3PSID	g.a000uggwjpjV8EfIwqzfQSYerkpCdpP9J97NW4akZcSosHPNlqB7_E_iCPYSIlJOVCtA4hpgqAACgYKAe4SARESFQHGX2Mity9SfGiiUsM1zyRh1nvyERoVAUF8yKqOVGEbsIFsDeK7xQdKeqo30076
.youtube.com	TRUE	/	TRUE	1772930082	__Secure-3PSIDCC	AKEyXzU_s8vF8z6T2c9n8MoZPk35nLa5-N7zm51hTcRcwspJzKKjVToHXShj1acZAivn3yPs0zk
.youtube.com	TRUE	/	TRUE	1772927402	__Secure-3PSIDTS	sidts-CjEBEJ3XV4zWBqC5YMY9TxOWiJp7U1ze9Pt9cX0Aefl1Gmn3RSx2xZb7o9T2z8sPHxTSEAA
.youtube.com	TRUE	/	FALSE	1775951402	APISID	lwr3FvZrIj5Mfuj9/AdV7HPmE64YDOZoKu
.youtube.com	TRUE	/	FALSE	1775951402	HSID	Aov0774MJViBYwZQb
.youtube.com	TRUE	/	TRUE	1775951403	LOGIN_INFO	AFmmF2swRQIhAMQ7A8Zm-f0BckVZLxcf8N0xdSjJivKdoRaCaCx_PE-TAiBCgmg8TiBfaeOyMgnX4xKNbT1h79K1XuK7vqetH34H-w:QUQ3MjNmenp3TW5yeVV0ZGU1cFZ5ZTRrYUkzUUF5WVhNdTZMcG0waUhXUWxvMmNIN3NsYk5ya3Nvank2QWpxRFJwSnQ1ZHM5ekN0cUZidjlWWEVqeEZxYnoxMnRMdEhzYnU3eTRvREd0eHV1T0g5cWJybkd0QmtYSkhDYml3UEJxcTJrR1VVT3NrWjh6Ums1UE1wVFp0WVQzbUhVemh6MVFn
.youtube.com	TRUE	/	FALSE	0	PREF	f4=4000000&f6=40000000&tz=UTC&hl=en
.youtube.com	TRUE	/	TRUE	1775951402	SAPISID	LaNWPo4ApXF469R7/AInNpn8a8Jb48eLib
.youtube.com	TRUE	/	FALSE	1775951402	SID	g.a000uggwjpjV8EfIwqzfQSYerkpCdpP9J97NW4akZcSosHPNlqB7FtnvfymX0B6SdnWxMC3EuQACgYKAV8SARESFQHGX2MiH5zNkgHF7F-zbQfb9BO01RoVAUF8yKokA8x8akRSA4qhFWsb-e4i0076
.youtube.com	TRUE	/	FALSE	1772930082	SIDCC	AKEyXzU-1YrNC3GKxspiLQNPpHjcBZQVIhRW1syLw-mq6Ucv9iZzKuja7DhpTMz7ej5A6RbVbQ
.youtube.com	TRUE	/	TRUE	1775519383	SOCS	CAISEwgDEgk3MzM4ODU4MTgaAmVuIAEaBgiA7qi-Bg
.youtube.com	TRUE	/	TRUE	1775951402	SSID	A3uDuql8vV_WXDhT0
.youtube.com	TRUE	/	TRUE	1756946082	VISITOR_INFO1_LIVE	qtWZHMitiUk
.youtube.com	TRUE	/	TRUE	0	YSC	YqklUmNAerM
.youtube.com	TRUE	/	TRUE	1756943584	__Secure-ROLLOUT_TOKEN	COPi8oT2gYuakAEQyZialpX5iwMY09u5lpX5iwM%3D
.youtube.com	TRUE	/	TRUE	1756946082	VISITOR_PRIVACY_METADATA	CgJOTBIcEhgSFhMLFBUWFwwYGRobHB0eHw4PIBAREiEgXg%3D%3D
.youtube.com	TRUE	/	TRUE	1804466082	__Secure-YT_TVFAS	t=483719&s=2
.youtube.com	TRUE	/	TRUE	1756946082	DEVICE_INFO	ChxOelEzT1RJeE9Ua3dORGswT0RZeU1qQTRNdz09EKKhrr4GGOCNrr4G

================
File: source/data_acquisition/download-data.py
================
import yt_dlp
import os
import argparse
import stat
import re
import multiprocessing
import time

def sanitize_filename(filename):
    """
    Sanitize the filename by replacing spaces with underscores and removing special characters.

    Parameters:
    filename (str): The original filename to be sanitized.

    Returns:
    str: The sanitized filename.
    """
    # Replace spaces with underscores and remove special characters
    filename = filename.replace(' ', '_')
    return re.sub(r'[^a-zA-Z0-9_]', '', filename)

def download_video_and_chat(url, output_path, index, cookies_file):
    """
    Download a YouTube video and its live chat, saving them to a specified directory.

    Parameters:
    url (str): The URL of the YouTube video.
    output_path (str): The base directory where the video and chat will be saved.
    index (int): An index used to create a unique directory for each video.
    cookies_file (str): Path to the cookies file for authentication.

    Returns:
    None
    """
    # Create a directory for each video using an integer index
    video_dir = os.path.join(output_path, str(index))
    raw_dir = os.path.join(video_dir, 'raw')

    # Check if "done.txt" exists in the raw directory
    done_file_path = os.path.join(raw_dir, "done.txt")
    if os.path.exists(done_file_path):
        print(f"Skipping download for {index} as 'done.txt' exists.")
        return
    else:
        # Remove the contents of the raw folder if it exists
        if os.path.exists(raw_dir):
            for file in os.listdir(raw_dir):
                file_path = os.path.join(raw_dir, file)
                try:
                    if os.path.isfile(file_path) or os.path.islink(file_path):
                        os.unlink(file_path)
                    elif os.path.isdir(file_path):
                        os.rmdir(file_path)
                except Exception as e:
                    print(f"Failed to delete {file_path}. Reason: {e}")

    if not os.path.exists(raw_dir):
        os.makedirs(raw_dir)

    # Set permissions to 777 for the directories
    os.chmod(video_dir, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
    os.chmod(raw_dir, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)

    ydl_opts = {
        'outtmpl': os.path.join(raw_dir, '%(title)s.%(ext)s'),
        'writesubtitles': True,
        'writeautomaticsub': True,
        'subtitleslangs': ['en', 'live_chat'],
        'skip_download': False,
        'format': 'bestvideo[ext=mp4][height<=1080]+bestaudio[ext=m4a]/best[ext=mp4][height<=1080]',  # Ensure Full HD, MP4, and AAC audio
        'cookiefile': cookies_file,
        'sleep_interval_subtitles': 1,
        'sleep_interval': 3,
    }
    
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info_dict = ydl.extract_info(url, download=False)
        title = info_dict.get('title', None)
        ydl.download([url])
        print(f"Downloaded video and chat for: {index}")

        # Set permissions to 777 for the downloaded files
        video_file = os.path.join(raw_dir, f"{title}.mp4")
        chat_file = os.path.join(raw_dir, f"{title}.live_chat.json")
        subtitle_file = os.path.join(raw_dir, f"{title}.en.vtt")
        
        if title:
            title_new = sanitize_filename(title)

        if os.path.exists(video_file):
            os.chmod(video_file, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
            os.rename(video_file, os.path.join(raw_dir, f"{title_new}.mp4"))
            
        if os.path.exists(chat_file):
            os.chmod(chat_file, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
            os.rename(chat_file, os.path.join(raw_dir, f"{title_new}.live_chat.json"))

        if os.path.exists(subtitle_file):
            os.chmod(subtitle_file, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
            os.rename(subtitle_file, os.path.join(raw_dir, f"{title_new}.en.vtt"))

        # Check if all files have been renamed and create an empty "done" file
        renamed_video_file = os.path.join(raw_dir, f"{title_new}.mp4")
        renamed_chat_file = os.path.join(raw_dir, f"{title_new}.live_chat.json")
        renamed_subtitle_file = os.path.join(raw_dir, f"{title_new}.en.vtt")

        if os.path.exists(renamed_video_file) and os.path.exists(renamed_chat_file) and os.path.exists(renamed_subtitle_file):
            done_file_path = os.path.join(raw_dir, "done.txt")
            open(done_file_path, 'w').close()  # Create an empty file

def download_video_info(url, output_path, index, cookies_file):
    """
    Download the info JSON for a YouTube video and save the video link in 'done.txt'.

    Parameters:
    url (str): The URL of the YouTube video.
    output_path (str): The base directory where the info JSON will be saved.
    index (int): An index used to create a unique directory for each video.
    cookies_file (str): Path to the cookies file for authentication.

    Returns:
    None
    """
    # Create a directory for each video using an integer index
    video_dir = os.path.join(output_path, str(index))
    raw_dir = os.path.join(video_dir, 'raw')

    # Check if "done.txt" exists in the raw directory
    done_file_path = os.path.join(raw_dir, "done.txt")
    if not os.path.exists(done_file_path):
        print(f"Skipping download for {index} as 'done.txt' does not exist.")
        return

    ydl_opts = {
        'outtmpl': os.path.join(raw_dir, '%(title)s'),
        'skip_download': True,
        'writeinfojson': True,
        'cookiefile': cookies_file,
    }
    
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info_dict = ydl.extract_info(url, download=False)
        title = info_dict.get('title', None)
        ydl.download([url])
        print(f"Downloaded info JSON for: {index}")

        if title:
            title_new = sanitize_filename(title)
            original_info_file = os.path.join(raw_dir, f"{title}.info.json")
            sanitized_info_file = os.path.join(raw_dir, f"{title_new}.info.json")
            
            if os.path.exists(original_info_file):
                os.rename(original_info_file, sanitized_info_file)
                os.chmod(sanitized_info_file, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
                
        # Write the URL to the done file
        with open(done_file_path, 'w') as done_file:
            done_file.write(url + '\n')
        
        os.chmod(done_file_path, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)

def main():
    """
    Main function to parse command-line arguments and initiate the download process.

    Returns:
    None
    """
    parser = argparse.ArgumentParser(description='Download YouTube videos and chat history.')
    parser.add_argument('input_file', type=str, help='Path to the text file containing video URLs')
    parser.add_argument('--cookies_file', type=str, default='./cookies.txt', help='Path to the cookies file for authentication (optional)')
    parser.add_argument('--output_path', type=str, default='/mnt-persist/data', help='Output path for downloaded files (optional)')
    args = parser.parse_args()

    if not os.path.exists(args.output_path):
        os.makedirs(args.output_path)
        
    os.chmod(args.output_path, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
    
    with open(args.input_file, 'r') as file:
        urls = file.readlines()

    for index, url in enumerate(urls, start=1):
        url = url.strip()
        if url:
            try:
                #download_video_and_chat(url, args.output_path, index, args.cookies_file)
                download_video_info(url, args.output_path, index, args.cookies_file)
                time.sleep(3)
            except Exception as e:
                print(f"Failed to process {e}: {url}")

    # with open(args.input_file, 'r') as file:
    #     urls = [url.strip() for url in file.readlines() if url.strip()]

    # # Use multiprocessing to download videos concurrently
    # with multiprocessing.Pool(3) as pool:  # Reduce the number of concurrent processes
    #     pool.starmap(download_video_and_chat, [(url, args.output_path, index, args.cookies_file) for index, url in enumerate(urls, start=1)])

if __name__ == "__main__":
    main()

================
File: source/data_acquisition/preprocess-videos.py
================
import os
import subprocess
import argparse
import stat
import multiprocessing

# Function to process video files
def process_videos(base_dir, video_resolution, video_fps):
    """
    Process video files in the specified base directory.

    This function extracts images from video files at a specified resolution and frame rate,
    and extracts audio to MP3 format. It also sets permissions for the extracted files.

    Parameters:
    base_dir (str): The base directory containing video directories.
    video_resolution (str): The resolution for the extracted images in WxH format.
    video_fps (str): The frames per second for image extraction.
    """
    audio_files = []
    for video_dir in [f.path for f in os.scandir(base_dir) if f.is_dir()]:
        raw_dir = os.path.join(video_dir, 'raw')
        images_dir = os.path.join(video_dir, 'images')
        os.makedirs(images_dir, exist_ok=True)
        os.chmod(images_dir, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
        
        for file in os.listdir(raw_dir):
            if file.endswith(('.mp4', '.avi', '.mov')):  # Add more video formats if needed
                video_path = os.path.join(raw_dir, file)
                audio_mp3_path = os.path.join(video_dir, f'{os.path.splitext(file)[0]}.mp3')
                image_pattern = os.path.join(images_dir, f'{os.path.splitext(file)[0]}_%06d.jpg')

                print(f"Processing video: {video_path}")

                audio_files.append((video_path, audio_mp3_path))
                
                print(f"Extracting images to: {images_dir}")
                subprocess.run(['ffmpeg', '-i', video_path, '-vf', f'fps={video_fps}, scale={video_resolution}, format=yuv420p', '-threads', '16', '-thread_type', 'slice', image_pattern], check=True)

                # Rename images to reflect when the picture was taken from the video
                for image_file in os.listdir(images_dir):
                    if image_file.endswith('.jpg'):
                        image_path = os.path.join(images_dir, image_file)
                        # Extract the timestamp from the image filename
                        frame_number = int(image_file.split('_')[-1].split('.')[0])
                        # Calculate the timestamp in seconds
                        timestamp = int(frame_number / float(video_fps)) - 30 #there is an offset :((
                        # Format the timestamp as seconds
                        timestamp_formatted = f"{timestamp:06}"
                        # Create the new filename with the timestamp
                        new_image_file = f"{timestamp_formatted}_{os.path.splitext(file)[0]}.jpg"
                        new_image_path = os.path.join(images_dir, new_image_file)
                        # Rename the image file
                        os.rename(image_path, new_image_path)

                # Set permissions for all images recursively
                for root, dirs, files in os.walk(images_dir):
                    for image_file in files:
                        image_path = os.path.join(root, image_file)
                        os.chmod(image_path, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
                
                print(f"Set permissions for images: {images_dir}")
        
    # Use multiprocessing to process audio files in parallel
    with multiprocessing.Pool(8) as pool:
        pool.map(process_audio, audio_files)

def process_audio(inputs):
    """
    Process audio extraction from video files.

    This function extracts audio from video files and converts it to MP3 format.
    It also sets permissions for the extracted audio files.

    Parameters:
    inputs (tuple): A tuple containing the input video file path and the output audio file path.
    """
    audio_in, audio_out = inputs
    print(f"Processing audio: {audio_out}")
    subprocess.run(['ffmpeg', '-i', audio_in, '-q:a', '0', '-map', 'a', '-ar', '44100', '-ac', '1', '-threads', '1', audio_out], check=True)
    os.chmod(audio_out, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Process videos to extract audio and images.')
    parser.add_argument('base_dir', nargs='?', default='/mnt-persist/data', type=str, help='The base directory containing the data folders.')
    parser.add_argument('--video_resolution', type=str, default='1920x1080', help='Video resolution in WxH format.')
    parser.add_argument('--video_fps', type=str, default='0.016666666', help='Video fps.')
    args = parser.parse_args()

    print(f"Starting processing in directory: {args.base_dir}")
    process_videos(args.base_dir, args.video_resolution, args.video_fps)
    print("Processing complete.")

================
File: source/data_acquisition/pyproject.toml
================
[tool.poetry]
name = "byborgai"
version = "0.1.0"
description = ""
authors = ["Lovasz Botond <botilovasz@gmail.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.12"
yt-dlp = "^2025.2.19"
streamlit = "^1.43.1"
plotly = "^6.0.0"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

================
File: source/data_acquisition/videos.txt
================
https://www.youtube.com/watch?v=G4JoDcsk62A
https://www.youtube.com/watch?v=xDHIA-Wi58U
https://www.youtube.com/watch?v=fGXdUp9rfHo
https://www.youtube.com/watch?v=Wnptqz_177E
https://www.youtube.com/watch?v=CBfGIR0NqCE
https://www.youtube.com/watch?v=oUoO3Ik8aIk
https://www.youtube.com/watch?v=ZDLbakOm3mM
https://www.youtube.com/watch?v=k-T8L-WIGXQ
https://www.youtube.com/watch?v=fDkWNgmKCcE
https://www.youtube.com/watch?v=P8BEDv_p8TQ
https://www.youtube.com/watch?v=Btlttmh-xpc
https://www.youtube.com/watch?v=9EjryjlAr1w
https://www.youtube.com/watch?v=FwwgBB8l2vs
https://www.youtube.com/watch?v=EXBGRxmLqOU
https://www.youtube.com/watch?v=XmD9tUWbAV8
https://www.youtube.com/watch?v=ZAovoEHJ0Ec
https://www.youtube.com/watch?v=vhx1iphnaNQ
https://www.youtube.com/watch?v=LOU9Ea60Zzc
https://www.youtube.com/watch?v=MyAXV2TW_C4
https://www.youtube.com/watch?v=nETQGtt024k
https://www.youtube.com/watch?v=lhhKVq0uYIo
https://www.youtube.com/watch?v=-tPwmK5CZUg
https://www.youtube.com/watch?v=CY3OQh-7wIk
https://www.youtube.com/watch?v=emlRd643NVI
https://www.youtube.com/watch?v=LgoPHaaLtxk
https://www.youtube.com/watch?v=OVBJbW6pDVQ
https://www.youtube.com/watch?v=M7Lr3ujt4Ro
https://www.youtube.com/watch?v=ZK43RAc90ZA
https://www.youtube.com/watch?v=DRHcTfTfEp0
https://www.youtube.com/watch?v=mw2oDtcvYy8

================
File: source/feature_extraction/audio/extract.py
================
import os
import sys
import re
import _pickle as pickle
import numpy as np
import pandas as pd
import librosa
import torch
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM

if len(sys.argv) < 2:
    print("Usage: extract.py path_to_mp3 [--debug]")
    sys.exit(1)
path = sys.argv[1]
debug = "--debug" in sys.argv

model_id = "openai/whisper-large-v3-turbo"
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    use_safetensors=True,
    attn_implementation="eager",
)
model.to("cuda")
processor = AutoProcessor.from_pretrained(model_id)
pipe = pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    torch_dtype=torch.float16,
    device="cuda",
    return_timestamps="word",
)

def transcribe_audio(input_file):
    audio, _ = librosa.load(input_file, sr=44100)
    audio_16k = librosa.resample(audio, orig_sr=44100, target_sr=16000)
    
    # Split audio into 10-minute segments
    segment_duration = 10 * 60  # 10 minutes in seconds
    duration_seconds = int(librosa.get_duration(y=audio, sr=44100))
    num_segments = duration_seconds // segment_duration

    log_file = ""
    cumulative_offset = 0  # Initialize cumulative offset for timestamps
    for i in range(num_segments):
        start = i * segment_duration
        end = start + segment_duration
        segment = audio_16k[start * 16000:end * 16000]  # 16000 is the target sample rate
        
        result = pipe(segment)
        
        if 'chunks' in result:
            for chunk in result['chunks']:
                word = chunk['text']
                start_time, end_time = chunk['timestamp']
                adjusted_start_time = start_time + cumulative_offset
                adjusted_end_time = end_time + cumulative_offset
                log_file += f"Word: '{word}' | Start: {adjusted_start_time:.2f}s | End: {adjusted_end_time:.2f}s\n"
        else:
            log_file += "No timestamps available.\n"
        
        # Update cumulative offset by the segment duration
        cumulative_offset += segment_duration
    return log_file

print("stage1")
log_file = transcribe_audio(path)
del pipe
del processor
del model

def to_sentences(file_path):
    lines = file_path.split("\n")

    sentences = []
    current_sentence = []
    start_time = None
    end_time = None

    for line in lines:
        if len(line) == 0:
            continue
        match = re.match(r"Word: '(.+?)' \| Start: ([\d.]+)s \| End: ([\d.]+)s", line)
        if match:
            word, start, end = match.groups()
            start, end = float(start), float(end)

            if not current_sentence:
                start_time = start

            current_sentence.append(word)
            end_time = end

            # Check if the word ends with a sentence-ending punctuation
            if word.endswith(('.', '!', '?')) or (len(current_sentence) > 1 and current_sentence[-2].endswith(('.', '!', '?'))):
                sentences.append((start_time, end_time, ' '.join(current_sentence)))
                current_sentence = []

    # Handle any remaining sentence
    if current_sentence:
        sentences.append((start_time, end_time, ' '.join(current_sentence)))

    # Output the sentences in the desired format and append to processed_sentences.txt
    output_file = ""
    for start, end, sentence in sentences:
        output_file += f"[{start:.2f} - {end:.2f}] {sentence}\n"
    return output_file

print("stage2")
output_file = to_sentences(log_file)

model_name = "tabularisai/multilingual-sentiment-analysis"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
model.to("cuda")

def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to("cuda")
    with torch.no_grad():
        outputs = model(**inputs)
    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
    return torch.argmax(probabilities, dim=-1).item()

def add_sentiment(file_path):
    lines = file_path.split("\n")

    result = []
    for line in lines:
        if len(line) == 0:
            continue
        # Extract the sentence part (assuming timecode is at the start)
        timecode, sentence = line.split(']', 1)
        sentence = sentence.strip()
        start, end = timecode[1:].split(' - ')
        start = int(float(start))
        end = int(float(end))
        sentiment = predict_sentiment(sentence)
        result.append([start,end,sentiment,sentence])
    return np.array(result)

print("stage3")
result = add_sentiment(output_file)
np.save(path[:-4]+"_audio_raw.npy",result)
del model
del tokenizer

CONTEXT_L = 0
CONTEXT_S = 40
STEP = 60
OFFSET = 30

raw = np.load(path[:-4]+"_audio_raw.npy")
END = int(raw[-1,1])

PROMPT_L = ""
PROMPT_S = ""
PROMPT = ""

chunks = []
for i in range(END//STEP):
    end_s = OFFSET+i*STEP+STEP
    start_s = end_s-CONTEXT_S
    end_l = start_s
    start_l = end_s-CONTEXT_L
    context_l = ""
    context_s = ""
    for s in raw:
        st = int(s[0])
        if start_l <= st and st < end_l:
            context_l += "\""+s[3]+"\"\n"
        elif start_s <= st and st <= end_s:
            context_s += s[3]+"\n"
    chunk = ""
    if len(context_l) > 0:
        chunk += PROMPT_L
        chunk += context_l
    chunk += PROMPT_S
    chunk += context_s
    chunk += PROMPT
    chunks.append(chunk)

def pickle_load(path):
    with open(path,'rb') as f:
        ret = pickle.load(f)
    return ret

def pickle_save(path, obj):
    with open(path,'wb') as f:
        pickle.dump(obj,f)

def text_save(path, txt):
    with open(path,'w') as f:
        f.write(txt)

def get_closest_idx(items, candidate):
    m = 0
    ret_idx = 0
    idx = 0
    for item in items:
        dist = jaro.jaro_winkler_metric(item, candidate)
        if dist > m:
            m = dist
            ret_idx = idx
        idx += 1
    return ret_idx

def generate_onehot(items):
    prepromt = "Answer from the following list with only using a word from it ["+",".join(items)+"]!"
    closest = lambda a:get_closest_idx(items,a)
    return (prepromt, closest)

preprompts = {
    "list[str]":("Answer with a short, comma separated list!",lambda a:[re.sub("[\\s]+","_",b.strip().lower()) for b in a.split(',') if b.strip() != '']),
    "list[int]":("Answer with a short, comma separated list!",lambda a:[int(re.sub("[^\\d]","",b)) for b in a.split(',') if len(b.strip())>0]),
    "list[float]":("Answer with a short, comma separated list!",lambda a:[float(re.sub("[^\\d\\.]","",b)) for b in a.split(',') if b.strip() != '']),
    "int":("Answer with a single integer number!",lambda a:int(re.sub("[^\\d]","",a)) if len(re.sub("[^\\d]","",a))>0 else 0),
    "float":("Answer with a single float number!",lambda a:float(re.sub("[^\\d\\.]","",a)) if len(re.sub("[^\\d\\.]","",a))>0 else 0.0),
    "bool":("Answer with a single yes or no!",lambda a:"y" in a),
}

# preprompts = {
#     "list[str]":("Answer with a short, comma separated list!",lambda a:a),
#     "list[int]":("Answer with a short, comma separated list!",lambda a:a),
#     "list[float]":("Answer with a short, comma separated list!",lambda a:a),
#     "int":("Answer with a single integer number!",lambda a:a),
#     "float":("Answer with a single float number!",lambda a:a),
#     "bool":("Answer with a single yes or no!",lambda a:a),
# }

def get_preprompt(key):
    if isinstance(key, str):
        return preprompts[key]
    if isinstance(key, list):
        return generate_onehot(key)
    raise Exception(":))))")

prompts = [
    # People & Speaking Patterns
    ("int", "How many times does a speaker interrupt another speaker?"),
    ("int", "How many times does a speaker use humor or sarcasm?"),
    ("int", "How many times do speakers address each other by name?"),
    ("int", "How many times do speakers address the audience directly?"),
    ("int", "How often do speakers talk over each other?"),
    ("int", "How frequently do speakers express excitement or enthusiasm?"),
    ("int", "How many rhetorical questions are asked in the transcript?"),
    # Topics & Technical Content
    ("int", "How many times is 'PC build' or 'building' mentioned?"),
    # ("list[str]", "Which specific computer components are mentioned (CPU, GPU, RAM, SSD, etc.)?"),
    # ("list[str]", "Which specific PC brands are mentioned (Intel, AMD, NVIDIA, ASUS, etc.)?"),
    ("int", "How many times do they mention benchmarks or performance comparisons?"),
    ("bool", "Did they mention troubleshooting or fixing an issue?"),
    ("int", "How often do they mention overclocking or optimization?"),
    # ("list[str]", "Which specific software tools or BIOS settings are mentioned?"),
    ("int", "How many times do they mention price or cost?"),
    ("int", "How often do they discuss PC aesthetics (RGB lighting, case design, etc.)?"),
    # Jokes, Reactions & Engagement Hooks
    ("int", "How many times does someone say something funny?"),
    ("int", "How often do speakers exaggerate for comedic effect?"),
    ("int", "How many times is a running joke referenced?"),
    ("int", "How many times does Linus make a self-deprecating joke?"),
    # ("list[str]", "What emotions do speakers express when reacting with surprise or frustration?"),
    ("int", "How often do speakers break the fourth wall (acknowledging the audience or video production)?"),
    # ("list[str]", "Which internet slang or memes do speakers use?"),
    ("int", "How often does Linus joke about dropping something?"),
    ("int", "How often do they make a reference to previous LTT videos?"),
    ("int", "How many times do they tease each other or engage in friendly banter?"),
    # Instructions & Step-by-Step Explanations
    ("int", "How many times do they use words like 'step,' 'next,' or 'now we'?"),
    ("int", "How many times does Linus give direct instructions?"),
    ("int", "How frequently do they explain the reasoning behind a step?"),
    ("int", "How many times do they mention safety precautions (ESD, handling delicate parts, etc.)?"),
    ("int", "How many times do they explain a concept in layman's terms?"),
    ("int", "How often do they reference 'best practices' or 'what you should do'?"),
    ("int", "How frequently do they correct themselves or change their approach?"),
    ("int", "How many times does a speaker express doubt about a step or decision?"),
    # Mistakes, Troubleshooting & Problem Solving
    ("int", "How many times does something go wrong in the build process?"),
    ("int", "How often do they acknowledge a mistake?"),
    ("int", "How many times do they try to troubleshoot an issue?"),
    ("int", "How frequently do they joke about things breaking or not working?"),
    ("int", "How many times do they mention something being more difficult than expected?"),
    ("int", "How often do they have to redo a step?"),
    # ("list[str]", "What last-minute changes do they make in the build?"),
    # ("list[str]", "What common mistakes do they mention viewers might make?"),
    ("int", "How often do they say 'we will fix it later' or something similar?"),
    # ("list[str]", "Which components or tools do they express frustration with?"),
    # Sponsorships, Branding & Call-to-Actions
    ("int", "How many times is a sponsor mentioned?"),
    ("int", "How many times does Linus explicitly read an ad or promotional message?"),
    # ("list[str]", "Which products are mentioned in a way that suggests sponsorship?"),
    ("int", "How often do they mention LTT store products?"),
    ("int", "How many times do they remind viewers to subscribe or like the video?"),
    ("int", "How frequently do they mention future videos or upcoming content?"),
    ("int", "How many times do they ask viewers for opinions in the comments?"),
    # ("list[str]", "Which external websites or resources do they mention?"),
    # ("list[str]", "Which other YouTube channels do they reference?"),
    ("int", "How many times do they promote their paid content (Floatplane, LTT Labs, etc.)?"),
]

torch.random.manual_seed(0)
model_path = "mistralai/Mistral-7B-Instruct-v0.3"
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True,
)
tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.pad_token = tokenizer.eos_token
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
)
# prompts = prompts[0:10]

def prompt(chunk, prompts):
    messages = []
    pres = []
    for prompt in prompts:
        pre = get_preprompt(prompt[0])
        pres.append(pre[1])
        txt = ""
        txt += "Given the following transcript from a LinusTechTips video, where they are usually building computers, extract "
        txt += prompt[1]
        txt += "and "
        txt += pre[0]+"\n"
        txt += chunk+"\n"
        txt += pre[0]+" "+prompt[1]
        messages.append([
            {"role":"system","content":"You are a data processing agent. You extract information from provided transcripts, which are wrapped in quotes."},
            {"role":"user","content":txt},
        ])
    results = pipe(messages, batch_size=len(prompts), max_new_tokens=10)
    for i in range(len(results)):
        results[i] = pres[i](results[i][0]['generated_text'][-1]["content"])
        # print(prompts[i][1]+": "+str(results[i]))
    return results

idx = path.rfind("/")
pat = path[:idx]
nam = path[idx+1:-4]

os.makedirs(pat+"/feature_audio", exist_ok=True)
if debug:
    pd.set_option('display.width',os.get_terminal_size().columns)
    os.makedirs(pat+"/feature_audio_debug", exist_ok=True)
def ljust(s):
    s = s.astype(str).str.strip()
    return s.str.ljust(s.str.len().max())

# chunks = chunks[50:52]

for i in range(len(chunks)):
    results = prompt(chunks[i], prompts)
    timestamp = i*STEP+STEP
    timestamp_formatted = f"{timestamp:06}"
    pickle_save(pat+"/feature_audio/"+timestamp_formatted+"_"+nam+".pkl",results)
    if debug:
        prs = np.array([[str(p[0]),str(p[1])] for p in prompts])
        prs = np.concatenate([prs,np.array([[str(r) for r in results]]).T],axis=1)
        df = pd.DataFrame(prs, columns=['datatype','prompt','value'])
        txt = df.apply(ljust).to_string(index=False,justify='left')
        text_save(pat+"/feature_audio_debug/"+timestamp_formatted+"_"+nam+".txt",txt)
        text_save(pat+"/feature_audio_debug/"+timestamp_formatted+"_"+nam+"_chunk.txt",chunks[i])
    print(str(i+1)+"/"+str(len(chunks)))

================
File: source/feature_extraction/audio/pyproject.toml
================
[tool.poetry]
name = "audio"
version = "0.1.0"
description = ""
authors = ["Your Name <you@example.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.12"
transformers = "^4.49.0"
torch = "^2.6.0"
accelerate = "^1.4.0"
numpy = "2.1"
pandas = "^2.2.3"
protobuf = "^6.30.0"
sentencepiece = "^0.2.0"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

================
File: source/feature_extraction/image/extract.py
================
import os
import sys
import re
import _pickle as pickle
import numpy as np
import pandas as pd
import jaro
import torch
from PIL import Image
from transformers import pipeline, LlavaProcessor, LlavaForConditionalGeneration

if len(sys.argv) < 2:
    print("Usage: extract.py path_to_folder [--debug]")
    sys.exit(1)
path = sys.argv[1]
debug = "--debug" in sys.argv

def pickle_load(path):
    with open(path,'rb') as f:
        ret = pickle.load(f)
    return ret

def pickle_save(path, obj):
    with open(path,'wb') as f:
        pickle.dump(obj,f)

def text_save(path, txt):
    with open(path,'w') as f:
        f.write(txt)

def get_closest_idx(items, candidate):
    m = 0
    ret_idx = 0
    idx = 0
    for item in items:
        dist = jaro.jaro_winkler_metric(item, candidate)
        if dist > m:
            m = dist
            ret_idx = idx
        idx += 1
    return ret_idx

def generate_onehot(items):
    prepromt = "Answer from the following list with only using a word from it ["+",".join(items)+"]!"
    closest = lambda a:get_closest_idx(items,a)
    return (prepromt, closest)

preprompts = {
    "list[str]":("Answer with a short, comma separated list!",lambda a:[re.sub("[\\s]+","_",b.strip().lower()) for b in a.split(',') if b.strip() != '']),
    "list[int]":("Answer with a short, comma separated list!",lambda a:[int(re.sub("[^\\d]","",b)) for b in a.split(',') if len(b.strip())>0]),
    "list[float]":("Answer with a short, comma separated list!",lambda a:[float(re.sub("[^\\d\\.]","",b)) for b in a.split(',') if b.strip() != '']),
    "int":("Answer with a single integer number!",lambda a:int(re.sub("[^\\d]","",a)) if len(re.sub("[^\\d]","",a))>0 else 0),
    "float":("Answer with a single float number!",lambda a:float(re.sub("[^\\d\\.]","",a)) if len(re.sub("[^\\d\\.]","",a))>0 else 0.0),
    "bool":("Answer with a single yes or no!",lambda a:"y" in a),
}

def get_preprompt(key):
    if isinstance(key, str):
        return preprompts[key]
    if isinstance(key, list):
        return generate_onehot(key)
    raise Exception(":))))")

prompts = [
    #people_and_actions
    ("list[str]","What are the people wearing in the picture?"),
    ("list[str]","What is each person doing in the picture?"),
    ("int","How many people are in the picture?"),
    ("int","How many people are standing?"),
    ("int","How many people are sitting?"),
    ("int","How many people are looking at the camera?"),
    ("int","How many people are smiling?"),
    ("int","How many people are using tools?"),
    ("int","How many people are talking?"),
    ("int","How many people are handling computer components?"),
    ("int","How many people appear to be reacting emotionally?"),
    ("int","How many people are making gestures with their hands?"),
    #objects_and_environment
    ("list[str]","What types of computer components are in the picture?"),
    ("list[str]","What tools are being used in the picture?"),
    ("int","How many computer components are visible in the picture?"),
    ("int","How many tools are visible in the picture?"),
    ("int","How many monitors are in the picture?"),
    ("int","How many keyboards are in the picture?"),
    ("int","How many mice are in the picture?"),
    ("int","How many cables are visible in the picture?"),
    ("int","How many RGB lights are visible in the picture?"),
    ("int","How many workbenches or tables are visible in the picture?"),
    ("int","How many cases or chassis are in the picture?"),
    ("int","How many boxes or packaging materials are visible in the picture?"),
    ("bool","Is a completed PC visible in the picture?"),
    ("bool","Is a disassembled PC visible in the picture?"),
    ("bool","Are there any brand logos visible in the picture?"),
    (["low","medium","high"],"How cluttered is the workspace in the picture?"),
    #camera_angles_and_framing
    ("int","How many faces are clearly visible?"),
    ("int","How many text elements are in the picture?"),
    ("int","How many different colors dominate the frame?"),
    ("bool","Is the image a close-up or wide shot?"),
    ("bool","Is there text overlay visible in the picture?"),
    (["top-down","side-view","front-facing"],"What is the camera angle?"),
    #engagement_and_expression
    ("int","How many people appear to be laughing?"),
    ("int","How many people appear to be explaining something?"),
    ("bool","Are there exaggerated facial expressions in the picture?"),
    ("bool","Are there any pointing gestures in the frame?"),
    ("bool","Is anyone making a surprised expression?"),
    ("bool","Is there a dramatic pose or action in the picture?"),
    #logos_and_branding
    ("list[str]","What brands are visible in the picture?"),
    ("int","How many visible brand logos are in the picture?"),
    ("bool","Is there an LTT logo in the frame?"),
    ("bool","Is there a sponsor logo visible in the frame?"),
    #textual_elements_and_graphics
    ("int","How many text elements are present in the frame?"),
    ("int","How many overlay graphics are present?"),
    ("bool","Is there an on-screen subtitle or caption in the frame?"),
    ("bool","Are there any highlighted elements or arrows in the frame?"),
    ("bool","Does the frame contain a thumbnail-style reaction face?"),
]

# prompts = prompts[0:6]

pipe = pipeline("image-text-to-text", model="llava-hf/llava-v1.6-mistral-7b-hf", torch_dtype=torch.float16)
pipe.model = torch.compile(pipe.model, mode="max-autotune")

def prompt(image_path, prompts):
    messages = []
    pres = []
    for prompt in prompts:
        pre = get_preprompt(prompt[0])
        pres.append(pre[1])
        txt = pre[0]+' '+prompt[1]
        messages.append([
            {
                "role": "user",
                "content": [
                    {"type": "image", "url": image_path},
                    {"type": "text", "text": txt},
                ],
            },
        ])
    results = pipe(text=messages, max_new_tokens=20, batch_size=len(prompts))
    for i in range(len(results)):
        results[i] = pres[i](results[i][0]['generated_text'][-1]['content'])
    return results

# print(prompt('/mnt-persist/test/1/images/000005_Our_New_4500_Workstation_PCs_for_Editing.jpg', prompts))

images = os.listdir(path+"/images")
images = sorted(images)

# for i in range(len(images)):
#     if i % 12 != 0:
#         os.remove(path+"/images/"+images[i])


# images = images[0:2]

os.makedirs(path+"/feature_video", exist_ok=True)
if debug:
    pd.set_option('display.width',os.get_terminal_size().columns)
    os.makedirs(path+"/feature_video_debug", exist_ok=True)
def ljust(s):
    s = s.astype(str).str.strip()
    return s.str.ljust(s.str.len().max())

for i in range(len(images)):
    results = prompt(path+"/images/"+images[i], prompts)
    pickle_save(path+"/feature_video/"+images[i][:-4]+".pkl",results)
    if debug:
        prs = np.array([[str(p[0]),str(p[1])] for p in prompts])
        prs = np.concatenate([prs,np.array([[str(r) for r in results]]).T],axis=1)
        df = pd.DataFrame(prs, columns=['datatype','prompt','value'])
        txt = df.apply(ljust).to_string(index=False,justify='left')
        text_save(path+"/feature_video_debug/"+images[i][:-4]+".txt",txt)
    print(str(i+1)+"/"+str(len(images)))

================
File: source/feature_extraction/image/pyproject.toml
================
[tool.poetry]
name = "image-prompt"
version = "0.1.0"
description = ""
authors = ["Your Name <you@example.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.12"
transformers = "^4.49.0"
torch = "^2.6.0"
pandas = "^2.2.3"
jaro-winkler = "^2.0.3"
pillow = "^11.1.0"
numpy = "^2.2.3"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

================
File: source/frontend/first_video_data.py
================
import re
import pandas as pd

================
File: source/frontend/frontend.py
================
import streamlit as st
import numpy as np
import pandas as pd
import plotly.express as px
import datetime
from scipy.signal import argrelextrema
import re 
from sklearn.preprocessing import MinMaxScaler
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Set Streamlit page configuration
st.set_page_config(page_title="Streamer Engagement Dashboard", layout="wide")

def convert_index_to_timestamp(indices, wl: int):
    print(indices)
    """Convert indices to timestamps for better readability."""
    seconds = [idx * wl for idx in indices]  # Assuming 20-second intervals
    timestamps = [str(datetime.timedelta(seconds=sec)) for sec in seconds]
    return timestamps, seconds  

# Load the datasets
chat_data = pd.read_csv("/mnt-persist/data/3/raw/The_Ultimate_500_Dollar_Gaming_PC.live_chat_labels.csv")
merged_video_labels = pd.read_csv("/mnt-persist/data/merged_video_labels.csv")  # This already has 'video_number'
merged_data = merged_video_labels[merged_video_labels['video_number']==3]

feature_importances = pd.read_csv("/home/mika/ByborgAI/source/frontend/top_features.csv")

# Compute rolling mean for smoothing
timestamps, seconds = convert_index_to_timestamp(chat_data.index, 20)
chat_data["rolling_mean"] = chat_data["score"].rolling(window=5).mean()
merged_data["rolling_mean"] = merged_data["score"].rolling(window=5).mean()
chat_data["timestamps"] = timestamps
chat_data["seconds"] = seconds
timestamps, seconds = convert_index_to_timestamp(range(len(merged_data)), 60)
merged_data["timestamps"] = timestamps
merged_data["seconds"] = seconds  # Add raw seconds for YouTube links

# **Detect Local Maxima (Engaging Moments)**
window_size = 5  # Adjustable window for peak detection

# Find local maxima (peaks)
local_max_indices = argrelextrema(chat_data["rolling_mean"].values, np.greater, order=10)[0]

# Filter significant peaks based on a threshold
engagement_threshold = np.percentile(chat_data["rolling_mean"].dropna(), 90)  # Top 10% peaks
key_moments = chat_data.iloc[local_max_indices]
key_moments = key_moments[key_moments["rolling_mean"] >= engagement_threshold]

# **Generate YouTube Links for Engaging Moments**
YOUTUBE_VIDEO_URL = "https://www.youtube.com/watch?v=G4JoDcsk62A&ab_channel=LinusTechTips"
key_moments["youtube_link"] = key_moments["seconds"].apply(lambda sec: f"{YOUTUBE_VIDEO_URL}&t={sec}")

# Title and description
st.title("üìä Streamer Dashboard")
st.markdown("""
**Analyze your engagement metrics easily!**
- See trends in audience interaction
- Identify key moments from your streams
- Click timestamps to jump to the best moments in the video!
""")

# Sidebar settings
with st.sidebar:
    st.header("‚öôÔ∏è Settings")
    window_size = st.slider("Rolling Mean Window Size", min_value=1, max_value=20, value=5)

    chat_data["rolling_mean"] = chat_data["score"].rolling(window=window_size).mean()
    
    # Update peaks after changing window size
    local_max_indices = argrelextrema(chat_data["rolling_mean"].values, np.greater, order=10)[0]
    key_moments = chat_data.iloc[local_max_indices]
    key_moments = key_moments[key_moments["rolling_mean"] >= engagement_threshold]
    key_moments["youtube_link"] = key_moments["seconds"].apply(lambda sec: f"{YOUTUBE_VIDEO_URL}&t={sec}")

# Create a Plotly figure for engagement score

# Create a subplot with a secondary y-axis
fig = make_subplots(specs=[[{"secondary_y": True}]])
# Add engagement score line (primary y-axis)
fig.add_trace(
    go.Scatter(
        x=chat_data["timestamps"], 
        y=chat_data["rolling_mean"], 
        mode="lines",
        name="Engagement Score",
        line=dict(color="blue")
    ),
    secondary_y=False
)

fig.update_xaxes(tickmode="linear", dtick=int(len(chat_data) / 10))
# Add engaging moments as clickable hover tooltips
# Add engaging moments (scatter points)
fig.add_trace(
    go.Scatter(
        x=key_moments["timestamps"], 
        y=key_moments["rolling_mean"], 
        mode="markers", 
        marker=dict(size=10, color="red", symbol="star"),
        name="Most Engaging Moments",
        customdata=key_moments["youtube_link"],
        hovertemplate="<b>Timestamp:</b> %{x}<br>" +
                      "<b>Engagement Score:</b> %{y}<br>" +
                      "<b><a href='%{customdata}' target='_blank'>Watch on YouTube ‚ñ∂Ô∏è</a></b><extra></extra>"
    ),
    secondary_y=False
)

# Sidebar: Let users choose which features to overlay
with st.sidebar:
    st.header("üìä Feature Selection")
    
    # Extract feature names from the importance file
    top_features = feature_importances.sort_values(by="importance", ascending=False)["feature"].tolist()
    
    # Allow users to select which features to overlay
    selected_features = st.multiselect("Select Features to Overlay:", top_features)

# Overlay selected features on secondary y-axis
for feature in selected_features:
    if feature in merged_data.columns:
        fig.add_trace(
            go.Scatter(
                x=merged_data["timestamps"], 
                y=merged_data[feature], 
                mode="lines", 
                name=f"{feature} (Overlay)",
                opacity=0.7
            ),
            secondary_y=True  # Set to secondary y-axis
        )

fig.update_xaxes(tickmode="linear", dtick=int(len(merged_data) / 10))  # Reduce number of ticks

# Display the combined engagement & selected features plot
st.subheader("üî• Engagement Score & Selected Features Over Time")
# Update layout for dual y-axis
fig.update_layout(
    title="üî• Engagement Score & Selected Features Over Time",
    xaxis=dict(title="Time"),
    yaxis=dict(title="Engagement Score", color="blue"),
    yaxis2=dict(title="Feature Values", overlaying="y", side="right", color="green"),
    legend=dict(x=0.01, y=1)
)

# Show plot in Streamlit
st.plotly_chart(fig, use_container_width=True)

# **Display key engaging moments with clickable YouTube links**
st.subheader("üéØ Most Engaging Moments")
for _, row in key_moments.iterrows():
    st.markdown(f"**[{row['timestamps']} - Watch on YouTube ‚ñ∂Ô∏è]({row['youtube_link']})**")

# Add a data table
st.subheader("üìÑ Data Preview")
st.dataframe(merged_data.tail(10))

================
File: source/frontend/top_features.csv
================
feature,importance
num_smiling,0.5076464555617797
num_brand_logos,0.38767633692900577
num_reacting_emotionally,0.10467720750921453

================
File: source/label_extraction/extract_chat_metadata.py
================
import json
import pandas as pd
import argparse
import os
import datetime
import glob 

def convert_usec_to_seconds(timestamp_usec):
    """ Converts microseconds to seconds and returns an integer. """
    try:
        var = int(timestamp_usec) // 1000000 
        return var  # Convert ¬µs to seconds
    except (ValueError, TypeError):
        return None


def calculate_message_rate(df: pd.DataFrame, min_time: int, max_time: int, window_length: int = 60) -> list[dict]:
    total_time = max_time - min_time
    bucket_number = total_time // window_length

    buckets =  [min_time + i * window_length for i in range(bucket_number)]
    message_rate = {}
    distinct_author_rate = {}
    active_user_rates = {}
    for bucket in buckets:
        bucket_content = df[(df["times"] >= bucket) & (df["times"] < bucket + window_length)] 

        # Get length
        length = len(bucket_content)
        message_rate[bucket] = length
        
        # Get distinct authors
        distinct_author_rate[bucket] = bucket_content["author"].nunique()
        if message_rate[bucket] == 0:
            active_user_rates[bucket] = 0
        else:
            active_user_rates[bucket] = distinct_author_rate[bucket]/message_rate[bucket]
    return message_rate, distinct_author_rate, active_user_rates
        

def calculate_donation_rates(df: pd.DataFrame, min_time: int, max_time: int, window_length: int = 60) -> list[dict]:
    total_time = max_time - min_time
    bucket_number = total_time // window_length

    buckets =  [min_time + i * window_length for i in range(bucket_number)]
    donation_amount_rates = {}
    donation_rates={}
    for bucket in buckets:
        bucket_content = df[(df["times"] >= bucket) & (df["times"] < bucket + window_length)] 
        donation_amount_rates[bucket] = bucket_content["donationAmount"].sum()
        donation_rates[bucket] = len(bucket_content)
    return donation_amount_rates, donation_rates



def create_metadata_json(json_file, output_json, window_length, metadata_path=None, save_folder=None):
    """
    Extract chat messages and donation data from a JSON file and save them as a CSV.

    Parameters:
    - json_file: Path to the input JSON file.
    - output_json: Path to the output JSON file.
    """
    if not os.path.exists(json_file):
        print(f"Error: File '{json_file}' not found.")
        return

    with open(json_file, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]

    print("Extracting chat data...")

    extracted_data = []

    for entry in data:
        try:
            actions = entry.get("replayChatItemAction", {}).get("actions", [])
            for action in actions:
                chat_item = action.get("addChatItemAction", {}).get("item", {})

                # Check if it's a chat message
                if "liveChatTextMessageRenderer" in chat_item:
                    chat_data = chat_item["liveChatTextMessageRenderer"]
                    if chat_data.get("timestampText", {}).get("simpleText", "")[0] == "-":
                        continue
                    else:
                        timestamp_usec = chat_data.get("timestampUsec")
                        timestamp_sec = convert_usec_to_seconds(timestamp_usec) if timestamp_usec else None

                        extracted_data.append({
                            "messageType": "chatmessage",
                            "times": timestamp_sec,
                            "message": chat_data.get("message", {}).get("runs", [{}])[0].get("text", ""),
                            "donationAmount": None,
                            "author": chat_data.get("authorName", {}).get("simpleText", "")
                        })

                # Check if it's a donation message
                elif "liveChatPaidMessageRenderer" in chat_item:
                    donation_data = chat_item["liveChatPaidMessageRenderer"]
                    if donation_data.get("timestampText", {}).get("simpleText", "")[0] == "-":
                        continue
                    else:
                        timestamp_usec = donation_data.get("timestampUsec")
                        timestamp_sec = convert_usec_to_seconds(timestamp_usec) if timestamp_usec else None

                        amount_str = donation_data.get("purchaseAmountText", {}).get("simpleText", "")

                        extracted_data.append({
                            "messageType": "donation",
                            "times": timestamp_sec,
                            "message": donation_data.get("message", {}).get("runs", [{}])[0].get("text", ""),
                            "donationAmount": amount_str,
                            "author": donation_data.get("authorName", {}).get("simpleText", "")
                        })

        except Exception as e:
            print(f"Error processing entry: {e}")

    # Convert to DataFrame and save as json
    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(extracted_data, f, indent=4)

    print(f"Extraction complete. Data saved to {output_json}")
    print("Extracting rates of messages and donations")
    
    df = pd.read_json(output_json)

    min_time = df["times"].min()
    max_time = df["times"].max()

    chatmessage_df = df[df["messageType"]=="chatmessage"]

    donation_df = df[df["messageType"]=="donation"]

    

    message_rate, distinct_author_rate, active_user_rates = calculate_message_rate(df=chatmessage_df, 
                                                                                min_time=min_time, 
                                                                                max_time=max_time, 
                                                                                window_length=window_length)
    donation_amount_rates, donation_rates = calculate_donation_rates(df=donation_df, 
                                                                    min_time=min_time, 
                                                                    max_time=max_time,
                                                                    window_length=window_length)
    
    # Combine into a DataFrame
    df2 = pd.DataFrame.from_dict(
        {
            "message_rate": message_rate,
            "distinct_author_rate": distinct_author_rate,
            "active_user_rate": active_user_rates,
            # "donation_amount_rate": donation_amount_rates,
            "donation_rate": donation_rates,
        },
        orient="index"  # Ensures each dictionary is a row
    ).T  # Transpose so keys become index

    # Save CSV file in the processed directory
    labels_csv_path = os.path.join(
        save_folder,
        f"{os.path.splitext(os.path.basename(json_file))[0]}_labels.csv"
    )
 
    df2.index = df2.index - min_time


    # Normalize each column using min-max scaling
    df2 = (df2 - df2.min()) / (df2.max() - df2.min())
    
    if metadata_path != None:
        with open(metadata_path) as f:
            metadata = json.load(f)

        heatmap = metadata["heatmap"]

        def get_score_from_heatmap(time, heatmap):
            for dict in heatmap:
                # print(time)
                if (dict['start_time'] <= time) & (time < dict['end_time']):
                    return dict['value']
        
        df2["heatmap_value"] = df2.index.to_series().apply(lambda idx: get_score_from_heatmap(idx, heatmap))
        

        # Compute the weighted score
        df2["score"] = 0.5 * (0.8 * df2["message_rate"] + 0.2 * df2["donation_rate"]) + 0.5 * df2["heatmap_value"]
    else:
        df2["score"] = 0.8 * df2["message_rate"] + 0.2 * df2["donation_rate"]
    
    df2.to_csv(labels_csv_path)
    print(f"Labels saved to {labels_csv_path}")
    # Find the time steps with the highest score
    biggest_rows = df2.nlargest(5, "score")

    # print(biggest_rows["score"])

    for time in biggest_rows.index:
        print(str(datetime.timedelta(seconds=time)))



def main():
    parser = argparse.ArgumentParser(description="Extract chat metadata from JSON and save it as a cleaned JSON.")
    parser.add_argument("base_directory", type=str, help="Path to the base directory containing numbered folders")
    parser.add_argument("-wl", type=int, default=20, help="Length of window for calculating rates")
    args = parser.parse_args()

    base_directory = args.base_directory
    window_length = args.wl

    # Iterate over all numbered folders inside the base directory
    for folder in sorted(os.listdir(base_directory)):
        folder_path = os.path.join(base_directory, folder)

        # Check if it's a directory
        if os.path.isdir(folder_path):
            raw_folder = os.path.join(folder_path, "raw")

            # Process each JSON file in the raw folder
            json_files = glob.glob(os.path.join(raw_folder, "*live_chat.json"))
            info_json_files = glob.glob(os.path.join(raw_folder, "*info.json"))
            info_json_file = info_json_files[0] if info_json_files else None  

            for json_file in json_files:
                output_json = os.path.join(
                    raw_folder,
                    f"{os.path.splitext(os.path.basename(json_file))[0]}_clean.json"
                )
                print(f"Processing: {json_file} -> {output_json}")
                create_metadata_json(json_file, output_json, window_length, info_json_file, raw_folder)
    
if __name__ == "__main__":
    main()

================
File: source/training/merge_video_and_labels.py
================
import os
import pandas as pd
import numpy as np

def round_to_nearest_bucket(timestamp, bucket_size=20):
    return (timestamp // bucket_size) * bucket_size

def load_labels(video_number, base_path="/mnt-persist/data"): 
    # Locate the labels CSV file
    raw_path = os.path.join(base_path, str(video_number), "raw")
    labels_files = [f for f in os.listdir(raw_path) if f.endswith("_labels.csv")]
    
    if not labels_files:
        print(f"No labels file found for video {video_number}")
        return None
    
    labels_path = os.path.join(raw_path, labels_files[0])
    labels_df = pd.read_csv(labels_path)
    
    # Ensure the timestamp column exists (creating a synthetic one if missing)
    labels_df["timestamp"] = labels_df.index * 20  # Assume each row represents 20s interval
    labels_df["rounded_timestamp"] = labels_df["timestamp"].apply(round_to_nearest_bucket)
    
    # Keep only the score column
    labels_df = labels_df[["rounded_timestamp", "score"]]
    
    return labels_df

def merge_data(video_features_path):
    # Load video features
    video_df = pd.read_csv(video_features_path)
    video_df["rounded_timestamp"] = video_df["timestamp"].apply(round_to_nearest_bucket)
    
    merged_data = []
    for video_number in video_df["video_number"].unique():
        video_subset = video_df[video_df["video_number"] == video_number]
        labels_df = load_labels(video_number)
        
        if labels_df is None:
            continue
        
        merged_df = video_subset.merge(labels_df, on="rounded_timestamp", how="left")
        
        # Drop unwanted columns
        merged_df = merged_df.drop(columns=[col for col in merged_df.columns if "Unnamed" in col])
        merged_df = merged_df.drop(columns=["rounded_timestamp"], errors='ignore')
        
        merged_data.append(merged_df)
    
    final_df = pd.concat(merged_data, ignore_index=True)
    return final_df

# Path to video features CSV
video_features_csv = "/mnt-persist/data/video_features_one_hot.csv"
merged_df = merge_data(video_features_csv)

# Save the merged dataset
output_path = "/mnt-persist/data/merged_video_labels.csv"
merged_df.to_csv(output_path, index=False)

print(f"Merged dataset saved to {output_path}")

================
File: source/training/preprocess.py
================
import pickle
import os
import pandas as pd


def pickle_load(path):
    with open(path,'rb') as f:
        ret = pickle.load(f)
    return ret

# Ahh, der Ferrari... ein Name, der weltweit f√ºr Luxus, Leistung und italienische Handwerkskunst steht. Gegr√ºndet 1939 von Enzo Ferrari, hat sich das Unternehmen von einem bescheidenen Anfang zu einem globalen Symbol f√ºr Exzellenz und Innovation entwickelt.

def load_pickles_from_directory(directory_path: str, video_number: int):
    column_names = [
        "video_number", "timestamp", "wearing", "actions", "num_people", "num_standing", "num_sitting", "num_looking_at_camera",
        "num_smiling", "num_using_tools", "num_talking", "num_handling_components", "num_reacting_emotionally", "num_gesturing",
        "components", "tools", "num_components", "num_tools", "num_monitors", "num_keyboards", "num_mice", "num_cables", 
        "num_rgb_lights", "num_tables", "num_cases", "num_boxes", "pc_completed", "pc_disassembled", "has_brand_logos", "workspace_clutter",
        "num_faces_visible", "num_text_elements", "num_dominant_colors", "is_closeup", "has_text_overlay", "camera_angle",
        "num_laughing", "num_explaining", "has_exaggerated_expressions", "has_pointing_gesture", "has_surprised_expression", "has_dramatic_pose",
        "visible_brands", "num_brand_logos", "has_ltt_logo", "has_sponsor_logo",
        "num_text_elements", "num_overlay_graphics", "has_subtitle", "has_highlighted_elements", "has_thumbnail_reaction"
    ]
    
    results = {}
    for i, file in enumerate(os.listdir(directory_path)):
        timestamp = int(file[:6])
        results[i] = [video_number] + [timestamp] + pickle_load(os.path.join(directory_path, file))
    
    df = pd.DataFrame.from_dict(results, orient='index', columns=column_names)
    return df

def load_all_videos():
    finished_videos = [i + 1 for i in range(24) if i + 1 not in [14, 15, 17]]
    df_list = []
    for i in finished_videos:
        df_list.append(load_pickles_from_directory(f"/mnt-persist/data/{i}/feature_video", i))
    return pd.concat(df_list, ignore_index=True)


import pandas as pd

def one_hot_encode_list_columns(df, list_columns):
    """One-hot encode columns containing lists of strings."""

    # This resulted in 1800 columns so we just drop em
    for col in list_columns:
        if col in df.columns:
            df = df.drop(columns=[col])
    return df


Berlinetta = load_all_videos()
# Example usage
list_columns = ["wearing", "actions", "components", "tools", "visible_brands"]  # Adjust based on your actual data
Berlinetta = one_hot_encode_list_columns(Berlinetta, list_columns)
print(Berlinetta)
print(Berlinetta.iloc[0])
Berlinetta.to_csv("/mnt-persist/data/video_features_one_hot.csv")

================
File: source/training/train_rules.py
================
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.tree import plot_tree, export_text
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
import os

# Define save path
save_path = "./frontend/"
os.makedirs(save_path, exist_ok=True)  # Ensure directory exists

# Load Data
df = pd.read_csv("/mnt-persist/data/merged_video_labels.csv")
df.dropna(how='any', inplace=True)

# Convert engagement score into binary categories: Low (0) and High (1)
df['engagement_category'] = pd.cut(df['score'], bins=[-1, 0.5, 1], labels=['low', 'high'])

# Drop original engagement score and unrelated columns
df = df.drop(columns=['score', 'video_number', 'timestamp'])

# Define features and target
X = df.drop(columns=['engagement_category'])
y = df['engagement_category']

# Handle class imbalance (Oversample High Engagement)
df_high = df[df['engagement_category'] == 'high']
df_low = df[df['engagement_category'] == 'low']
df_high_upsampled = resample(df_high, replace=True, n_samples=len(df_low), random_state=42)
df_balanced = pd.concat([df_low, df_high_upsampled]).sample(frac=1, random_state=42)

# Features & Target after balancing
X = df_balanced.drop(columns=['engagement_category'])
y = df_balanced['engagement_category']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Decision Tree
tree = DecisionTreeClassifier(max_depth=2, min_samples_split=5, random_state=42)
tree.fit(X_train, y_train)

# Print tree rules
tree_rules = export_text(tree, feature_names=list(X.columns))
print(tree_rules)

# Feature Importance Analysis
feature_importance = pd.DataFrame({'feature': X.columns, 'importance': tree.feature_importances_})
feature_importance = feature_importance.sort_values(by='importance', ascending=False)
print("Top 3 features driving high engagement:")
print(feature_importance.head(3))

# Save top 3 most important features to a file
top_features_path = os.path.join(save_path, "top_features.csv")
feature_importance.head(3).to_csv(top_features_path, index=False)

================
File: source/pyproject.toml
================
[tool.poetry]
name = "source"
version = "0.1.0"
description = ""
authors = ["Agata Mosinska <mosinskaagata@gmail.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.12"
pandas = "^2.2.3"
streamlit = "^1.43.1"
plotly = "^6.0.0"
scipy = "^1.15.2"
streamlit-plotly-events = "^0.0.6"
matplotlib = "^3.10.1"
scikit-learn = "^1.6.1"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

================
File: .gitignore
================
.*
!/.gitignore
!/.gitattributes
__pycache__
data
test
yolo11x.pt
 
*.ldb

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# PyPI configuration file
.pypirc
/data

================
File: gitpush.sh
================
git add -A
git commit -m "$1"
git push

================
File: mkdocs.yml
================
site_name: "Synthesis Documentation"
theme:
  name: 'material'

plugins:
  - search
  - mkdocstrings:
      handlers:
        python:
          paths: ["source"]

  - api-autonav:
      modules:
        - "source/data_acquisition"
        - "source/feature_extraction/audio"
        - "source/feature_extraction/image"
        - "source/frontend"
        - "source/label_extraction"
        - "source/training"
      cli_scripts:
        - "source/data_acquisition/download-data.py"
        - "source/feature_extraction/audio/extract.py"
        - "source/feature_extraction/image/extract.py"
        - "source/frontend/frontend.py"
        - "source/label_extraction/extract_chat_metadata.py"
        - "source/training/merge_video_and_labels.py"
        - "source/training/train_rules.py"
      nav_item_prefix: ""
      nav_section_title: "API Reference"
      cli_nav_section_title: "CLI Reference"

markdown_extensions:
  - pymdownx.superfences
  - mkdocs-click

================
File: pyproject.toml
================
[tool.poetry]
name = "byborgai"
version = "0.1.0"
description = ""
authors = ["Andris Gyori <gyori.andris@gmail.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = ">=3.10"
mkdocs = "^1.6.1"
mkdocstrings = "^0.28.2"
mkdocs-api-autonav = "^0.2.1"
mkdocs-material = "^9.6.7"
mkdocs-click = "^0.8.1"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

================
File: README.md
================
# ByborgHackathon
Codebase for the 24H Byborg AI Hackathon

# Streamer Content Analysis Tool  

## Overview  

This project is a **retrospective analysis tool for streamers**, designed to help creators understand how different aspects of their content impact audience engagement. The system extracts structured features from both **audio and video**, correlates them with **viewer interaction metrics**, and uses an **explainable machine learning model** to uncover patterns that drive engagement.  

## Key Components  

### 1. Feature Extraction  
- **Audio Analysis**:  
  - Transcribes speech and extracts structured metrics using an LLM.  
  - Detects key elements such as:  
    - Frequency of audience engagement.  
    - Mentions of price or cost.  
    - Sponsorship mentions.  

- **Video Analysis**:  
  - Samples frames at regular intervals and processes them with a vision-language model.  
  - Extracts visual features such as:  
    - Number of faces in the frame.  
    - Presence of dramatic poses or actions.  
    - Visible objects (e.g., boxes in unboxing content).  

### 2. Engagement Labeling  
- Extracts audience interaction KPIs, including:  
  - **Chat activity rate**  
  - **Donation frequency**  
  - **Replay heatmap data**  
- These metrics are combined into a **single engagement score**.  

### 3. Explainable Machine Learning Model  
- Uses a **decision tree** to analyze correlations between content features and engagement.  
- Example insight: **Streams with more visible boxes tend to have higher engagement**, aligning with the popularity of unboxing content.  

## Why Use This Tool?  
This tool helps streamers refine their content strategy by providing **clear, data-driven insights** into what keeps their audience engaged.  

# Source
```
‚îú‚îÄ‚îÄ data_acquisition
‚îú‚îÄ‚îÄ feature_extraction
‚îÇ   ‚îú‚îÄ‚îÄ audio
‚îÇ   ‚îî‚îÄ‚îÄ image
‚îú‚îÄ‚îÄ label_extraction
‚îú‚îÄ‚îÄ training
‚îî‚îÄ‚îÄ frontend
```
### data_acquisition

#### download-data.py
This script downloads YouTube videos and their chat history based on a list of URLs provided in a text file.  
##### Prerequisites  
Make sure you have Python installed and the necessary dependencies (e.g., `argparse`, `yt-dlp`).  
##### Usage  
Run the script from the command line:  
```bash
python download-data.py <input_file> [--cookies_file <path>] [--output_path <path>]
```

#### preprocess-videos.py
This script processes video files by **extracting images** at a specified resolution and frame rate and **extracting audio** in MP3 format. It also ensures correct file permissions and uses multiprocessing to speed up audio processing.  
##### Prerequisites  
Ensure you have **FFmpeg** installed and accessible from the command line.  
##### Usage  
Run the script from the command line:  
```bash
python preprocess-videos.py [base_dir] [--video_resolution <WxH>] [--video_fps <fps>]
```

### feature_extraction - audio

##### Usage  
Run the script from the command line:  
```bash
python extract.py path_to_mp3 [--debug]
```
##### Features
- Transcription: Converts speech to text using OpenAI Whisper.
- Sentence Segmentation: Structures transcript into sentences with timestamps.
- Sentiment Analysis: Classifies each sentence's sentiment.
- Feature Extraction: Identifies specific characteristics such as interruptions, humor, and technical terms.
- Chunk Processing: Breaks transcript into segments for structured data extraction.
##### Output
- Processed transcript with timestamps and sentiment labels (*_audio_raw.npy).
- Extracted feature data saved as pickle files (feature_audio/).
- Debug logs (if enabled) stored in feature_audio_debug/.
##### Requirements
- Python 3.x
- torch, transformers, librosa, numpy, pandas
- Ensure a compatible GPU is available for optimal performance.

### feature_extraction - image

##### Usage  
Run the script from the command line:  
```bash
python extract.py path_to_folder [--debug]
```
##### Features
This script extracts structured features from images using a vision-language model (LLaVA) and saves the results in a structured format.
##### Output
- Processed feature data is saved as .pkl files in feature_video/.
- If --debug is enabled, additional human-readable text files are saved in feature_video_debug/.
##### Requirements
- Python 3.x
- torch, transformers, numpy, pandas, PIL, jaro
- Ensure a compatible GPU is available for optimal performance.

### label_extraction
This script processes chat data from JSON files, extracts messages and donation events, and calculates engagement metrics over time. The output is stored as cleaned JSON and CSV files for further analysis.
##### Usage  
Run the script from the command line:  
```bash
python script.py base_directory [-wl WINDOW_LENGTH]
```
##### Features
- Chat message and donation extraction: Converts raw chat logs into structured JSON format.
- Message rate (messages per time window)
- Distinct authors per window
- Active user rate (ratio of unique authors to total messages)
- Donation rate (number and amount of donations per window)
- Optional integration with metadata heatmaps for scoring high-engagement moments
##### Output
- Cleaned chat data: Saved as <filename>_clean.json inside each raw/ folder.
- Engagement metrics: Saved as <filename>_labels.csv
- Top 5 high-engagement timestamps: Printed in hh:mm:ss format.
##### Requirements
- Python 3.x
- pandas

### training

#### preprocess.py
This script loads serialized data from pickle files, processes video and audio metadata, and applies transformations for further analysis. The output is stored as a structured CSV file.
##### Output
A CSV file containing structured video and audio feature data

#### merge_video_and_labels.py
This script processes video and audio feature data and merges it with precomputed label scores to create a structured dataset for further analysis.
##### Output
A CSV file containing merged video and audio features and label scores

#### train_rules.py
This script processes video engagement data, balances class distribution, and trains a decision tree model to classify engagement levels as "low" or "high".
##### Output
- Decision tree rules printed to the console.
- Top 3 most important features saved to top_features.csv



================================================================
End of Codebase
================================================================
